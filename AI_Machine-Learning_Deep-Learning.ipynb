{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa551c73",
   "metadata": {},
   "source": [
    "# Implementing an artifical neuron from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6d9e3",
   "metadata": {},
   "source": [
    "Example of an artificial neural network and the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = 1/(1+ np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def activate(inputs, weights):\n",
    "    h = 0\n",
    "    \n",
    "    # Perform dot product\n",
    "    dot_prod = np.dot(inputs, weights) \n",
    "    \n",
    "    # Perform activation function Sigmoid\n",
    "    h = sigmoid(dot_prod)\n",
    "    \n",
    "    #Return h\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__==\"__main__\":\n",
    "inputs = [0.5, 0.3, 0.2]\n",
    "weights = [0.4, 0.7, 0.2]\n",
    "output = activate(inputs, weights) #Computational unit of the ANN.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150367e6",
   "metadata": {},
   "source": [
    "First neuron has been implemented from scratch in python. \n",
    "Artificial neurons are loosely inspired to biological neurons.\n",
    "Artificial neurons are computuational units and they transform inputs into outputs using an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e10ba",
   "metadata": {},
   "source": [
    "## Computation in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659df807",
   "metadata": {},
   "source": [
    "Introduction of the multi-layer perceptrion.\n",
    "    A single neuron works for linear problems.\n",
    "    Real-world problems are complex.\n",
    "    ANNs can reproduce highly non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c73ca44",
   "metadata": {},
   "source": [
    "The artificial neuron does two things:\n",
    "1. Performing the net or sum of all weights multiplied (dot product) of the inputs (w.x)\n",
    "2. Modulating the net input using an activation function (prediction function). a = f(h)\n",
    "\n",
    "Computation in MLP (1st layer or input layer):\n",
    "1. x is passed as an input vector.\n",
    "\n",
    "Computation in MLP (2nd layer or 1st hidden layer):\n",
    "1. h_(2nd layer) = np.dot(x, W_(1st layer))\n",
    "2. activation_function_(2nd_layer) = f(h_(2nd layer))\n",
    "\n",
    "Computation in MLP (3rd layer or 2nd hidden layer): \n",
    "1. h_(3rd layer) = np.dot(x, W_(2nd layer))\n",
    "2. activation_function_(3rd_layer) = f(h_(3rd layer))\n",
    "*and it continues with as many layers present in the topology of the artifical neural network you are working on.\n",
    "\n",
    "Computation in MLP(Last layer or output layer):\n",
    "1. h_(last layer) = np.dot(x, W_(n-1 layer))\n",
    "2. activation_function_(last_layer) = f(h_(last layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32340aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.8, 1]\n",
    "weights = [[1.2, 0.7, 1], \n",
    "           [2, 0.6, 1.8]]\n",
    "#output = activate(inputs, weights) #Computational unit of the ANN.\n",
    "hidden_output = activate(inputs, weights)\n",
    "print(hidden_output)\n",
    "\n",
    "#Weight matrix between hidden layer and output layer given\n",
    "weights_2 = [1, 0.9, 1.5]\n",
    "output = activate(hidden_output, weights_2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a23a69",
   "metadata": {},
   "source": [
    "## Implementing a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48c1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f410d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP: #MLP: Multi-Layer Perceptron\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3, 5], num_outputs=2):\n",
    "        #First hidden layer has 3 neurons/inputs and second hidden layer has 5 neurons/inputs\n",
    "        #Output has 2 outcomes, or categories that the prediction will fall into. \n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        #Internal representation of a hidden layers, as a list\n",
    "        #Each item in list represents # of neurons in a layer. Layer moves from 0 index to # of layers that we have.\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs] #Concatenating function variables with +\n",
    "        \n",
    "        #Initiate random weights; weights represent the connections and their connection strengths (hence the word weights).\n",
    "        self.weights = [] #Initializing weight vector\n",
    "        \n",
    "        #Iterating through all layers to create matrix weight for each pair of layers\n",
    "        for i in range(len(layers) - 1):\n",
    "            #Rows are the current layer that it's in and columns are number of neurons on the subsequent layer.\n",
    "            #We have all connection of a neuron from the previous layer in the rows with the subsequent/previous layer.\n",
    "            #Number of rows equal number of neurons in a layer and number of columns equal number of neurons in sub layer. \n",
    "            w = np.random.rand(layers[i], layers[i+1]) #2-D Array \n",
    "            self.weights.append(w) #Store weight matrixes; number of weight matrixes is equal to number of layers minus 1.\n",
    "            \n",
    "    def forward_propagate(self, inputs): #Do computation forward into the neural network. \n",
    "        activations = inputs #For 1st layer, activation is the inputs.\n",
    "        \n",
    "        for w in self.weights: #Loop through all weight matrixes which is looping through all layers in the network.\n",
    "            #Calculating net inputs of given layer\n",
    "            net_inputs = np.dot(activations, w) #Matrix multiplication of activation of previous layer with weight matrix. \n",
    "            \n",
    "            #Calculating activation of given layer\n",
    "            activations = self._sigmoid(net_inputs) #Passing net inputs to the sigmoid function\n",
    "            \n",
    "        return activations\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e244d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network input is: [0.55010556 0.73036645 0.10473932]\n",
      "The network output is: [0.8970457  0.86531052]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #create an MLP\n",
    "    mlp = MLP()\n",
    "    \n",
    "    #create some inputs\n",
    "    inputs = np.random.rand(mlp.num_inputs) #1-D array or vector; size is number of neurons receiving the inputs \n",
    "    \n",
    "    #perform forward prop; ouputs of previous layer is inputs of next layer\n",
    "    outputs = mlp.forward_propagate(inputs)\n",
    "    \n",
    "    #print the results\n",
    "    print(\"The network input is: {}\".format(inputs))\n",
    "    #print(f\"The network output is: {outputs}\")\n",
    "    print(\"The network output is: {}\".format(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e2fb5",
   "metadata": {},
   "source": [
    "## Training a neural network: Backward propagation and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1163473",
   "metadata": {},
   "source": [
    "1. Tweak all the weights of the connections among different neurons so that we can have predictions that are really good.\n",
    "2. How we do this is feed training data (input + target) to the network. \n",
    "3. Look at predictions and calculate the error between the prediction and the expected outcome.\n",
    "4. Calculate gradient of error function over weights. Gradient will tell us the direction of local minimum of loss function. \n",
    "5. We use this information to perform iterative adjustments to the weights via back propagation. \n",
    "6. Back propagation updates the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad176cea",
   "metadata": {},
   "source": [
    "The loss function or error function being used is E(p,y) = 1/2(p-y)^2.\n",
    "\n",
    "To calculate the gradient of the error function, we use: partial_derivate(E)/partial_derivate(W_nlayer). Therefore, the gradient of the quadratic error function is partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd31d8",
   "metadata": {},
   "source": [
    "In machine learning, a loss function and an optimizer are two essential components that help to improve the performance of a model. A loss function measures the difference between the predicted output of a model and the actual output, while an optimizer adjusts the model's parameters to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f0c15",
   "metadata": {},
   "source": [
    "F = F(x,W)\n",
    "\n",
    "E = E(p,y) = E(F(x,W),y)\n",
    "\n",
    "The error is a function of the weights itself. It makes sense to calculate the gradient of the error function of the weights since E depends on the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576dc77",
   "metadata": {},
   "source": [
    "## Backpropagation (Moving the error signal from right to left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a058eb",
   "metadata": {},
   "source": [
    "Going backwards, from the output function to the first layer or input layer of an ANN:\n",
    "1. Caculate the error on the prediction vs actual.\n",
    "2. Once we have the errors, we use them to calculate the first derivate of the error with respect to W_n-1_layer.\n",
    "3. We back propagate, or use this information, to caculate the derivate of the error function with respect to W_n-2_layer.\n",
    "4. This step is repeated as we move backwards in the ANN until we reach the input layer. The last calculation is done between the first hidden layer and the input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c787bda",
   "metadata": {},
   "source": [
    "## Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160fafc",
   "metadata": {},
   "source": [
    "To update parameters, we use a very important algorithm called gradient descent. This works by taking a step in the opposite direction to the gradient. The size of the step is the learning rate, a hyperparameter. \n",
    "\n",
    "What we mean when taking a step in the opposite direction to the gradient:\n",
    "1. Gradient on a graph is represented by a straight line pointing to the direction where the loss function increases. \n",
    "2. We want to minimize the loss/error function so we take learning steps in the opposite of the gradient. \n",
    "3. Steps are taken until we reach the global minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f455fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
