{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0dc2f82",
   "metadata": {},
   "source": [
    "## Repurposing Multi-Layer Perceptron class to include backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe04c94",
   "metadata": {},
   "source": [
    "#### Changes include saving the activations and derivatives to compute backpropagation. We are now:\n",
    "1. Implementing backpropagation\n",
    "2. Implementing gradient descent\n",
    "3. Implementing a higher level training method that uses backpropagation and gradient descent. \n",
    "4. Training artifical neural net with dummy dataset\n",
    "5. Making some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c64e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class MLP: #MLP: Multi-Layer Perceptron\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,3], num_outputs=2): #Default values\n",
    "        #First hidden layer has 3 neurons/inputs and second hidden layer has 5 neurons/inputs\n",
    "        #Output has 2 outcomes, or categories that the prediction will fall into. \n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        #Internal representation of a hidden layers, as a list\n",
    "        #Each item in list represents # of neurons in a layer. Layer moves from 0 index to # of layers that we have.\n",
    "        layers = [num_inputs] + num_hidden + [num_outputs] #Concatenating function variables with +\n",
    "        \n",
    "        #Initiate random weights; weights represent the connections and their connection strengths (hence the word weights).\n",
    "        weights = [] #Initializing weight vector\n",
    "        #Iterating through all layers to create matrix weight for each pair of layers\n",
    "        for i in range(len(layers) - 1):\n",
    "            #Rows are the current layer that it's in and columns are number of neurons on the subsequent layer.\n",
    "            #We have all connection of a neuron from the previous layer in the rows with the subsequent/previous layer.\n",
    "            #Number of rows equal number of neurons in a layer and number of columns equal number of neurons in sub layer. \n",
    "            w = np.random.rand(layers[i], layers[i+1]) #2-D Array \n",
    "            weights.append(w) #Appending weight values in the array for each given layer. Each layer has their own weight vector\n",
    "        self.weights = weights #Storing weight matrixes; number of weight matrixes is equal to number of layers minus 1.\n",
    "            \n",
    "        #Storing activations\n",
    "        activations = []\n",
    "        #Going through all the layers and creating dummy activiation array for each layer. \n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i]) #Amount of zeros equals the number of neurons that we have in each layer. 1-D array. \n",
    "            activations.append(a) #Each array in the list represents the activations for a given layer.\n",
    "        self.activations = activations #Storing this information in an instance variable called activations. \n",
    "        \n",
    "        #Storing derivatives\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1): #Derivatives taken with respect to weights and weights only appear between layers.\n",
    "            #Now expecting a 2-D array, which is a matrix.\n",
    "            d = np.zeros((layers[i], layers[i+1])) #(number of neurons in current layer, number of neurons in subsequent or next layer) \n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        \n",
    "    \n",
    "    def forward_propagate(self, inputs): #Computes forward propagation of the network based on input signals.  \n",
    "        activations = inputs\n",
    "        \n",
    "        #Saving activations of the first layer as the inputs we receive as an arg for forward propagation.\n",
    "        self.activations[0] = activations \n",
    "        \n",
    "        #Iterating through the network layers.\n",
    "        for i, w in enumerate(self.weights): #Loop through all weight matrixes which is looping through all layers in network.\n",
    "            #Calculating net inputs of given layer\n",
    "            net_inputs = np.dot(activations, w) #Matrix multiplication of activation of previous layer with weight matrix. \n",
    "            \n",
    "            #Calculating activation of given layer using sigmoid function\n",
    "            activations = self._sigmoid(net_inputs) #Passing net inputs to the sigmoid function\n",
    "            self.activations[i+1] = activations #Storing activation at i + 1 given it is used for the next layer. \n",
    "                                                #Ex: if i = 2; a_3 = s(h_3) and h_3 = a_2 * W_2      \n",
    "        return activations\n",
    "    \n",
    "    \n",
    "    #Implementing backpropagation using stored derivates of the error. \n",
    "    def back_propagate(self, error, verbose=False): #Implementing verbose mode. Verbose just means more wordy descriptions. \n",
    "        # dE/dW_i = (y - a_[i+1]) * s`(h_[i+1]) * a_i    (dE/dW = delta); (d = derivative); (s = sigma); (s` = sigma prime)\n",
    "        # s`(h_[i+1]) = s(h_[i+1])(1 - s(h_[i+1]))\n",
    "        # s(h_[i+1]) = a_[i+1]\n",
    "        \n",
    "        for i in reversed(range(len(self.derivatives))): #Going from right to left of the artifical neural net with reversed. \n",
    "            activations = self.activations[i+1] #Using activations in the subsequent or previous layers. \n",
    "            \n",
    "            delta = error * self._sigmoid_derivative(activations) #ndarray([0.1, 0.2]) --> ndarray([[0.1, 0.2]]); 2D array\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T #Transpose the 2D array with a single row to a single column.\n",
    "            \n",
    "            current_activations = self.activations[i] # Array needs to be reorganized to perform dot product for deriv_i below.\n",
    "            #ndarray([0.1, 0.2]) --> ndarray([[0.1], [0.2]]) #2D array with a single row.\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0], -1) \n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            \n",
    "            #Now calculating the next derivative going to the left by using the previous derivative\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "        \n",
    "            if verbose == True: #Implementing verbose mode where derivatives are printed.\n",
    "                print(\"Derivatives for W{}: {}\".format(i, self.derivatives[i]))\n",
    "        \n",
    "        return error #Returning error backpropagated back to the input layer.\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)): #Going through all the different weight matrixes\n",
    "            weights = self.weights[i] #Retrieving Weights for a given layer\n",
    "            #print(\"Original W{}: {}\".format(i, weights))\n",
    "            \n",
    "            derivatives = self.derivatives[i] #Retrieving relative derivatives for a given layer.\n",
    "            \n",
    "            weights += derivatives * learning_rate #Summing weight and derivative matrixes for given layer and applying lrn_rate\n",
    "            #print(\"Updated W{}: {}\".format(i, weights))\n",
    "    \n",
    "    \n",
    "    def train(self, inputs, targets, epochs, learning_rate): #Fitting inputs into the model for training.\n",
    "        #Epoch: After passing all the inputs in the dataset, an epoch is finished.\n",
    "        #The number of epochs describes how many times the ENTIRE dataset is fed into the network.\n",
    "        \n",
    "        for i in range(epochs): #Number of epochs to train for\n",
    "            sum_error = 0\n",
    "            #Using zip to unpack inputs and targets to receive the elements one-by-one. \n",
    "            for input, target in zip(inputs, targets): #Going through all the inputs and target variables.\n",
    "                    \n",
    "                #Doing forward propagation\n",
    "                output = self.forward_propagate(input)\n",
    "\n",
    "                #Calculating error\n",
    "                error = target - output\n",
    "\n",
    "                #Doing backward propagation\n",
    "                self.back_propagate(error)\n",
    "\n",
    "                #Applying gradient descent\n",
    "                self.gradient_descent(learning_rate)\n",
    "                \n",
    "                sum_error += self._mse(target, output)\n",
    "                \n",
    "            #return average error at each epoch\n",
    "            print(\"Error: {} at epoch {}\".format(sum_error / len(inputs), i))\n",
    "    \n",
    "    def _mse(self, target, output):\n",
    "        return np.average((target - output)**2) #Returning mean squared error\n",
    "    \n",
    "    \n",
    "    def _sigmoid(self, x): #Activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def _sigmoid_derivative(self, x): #Derivative of activation function\n",
    "        return x * (1.0 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d6da6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.04699608109464281 at epoch 0\n",
      "Error: 0.04001267241563012 at epoch 1\n",
      "Error: 0.0396049213346582 at epoch 2\n",
      "Error: 0.039122848677907905 at epoch 3\n",
      "Error: 0.0385319468204186 at epoch 4\n",
      "Error: 0.0377935128973421 at epoch 5\n",
      "Error: 0.03686465107541399 at epoch 6\n",
      "Error: 0.0356996732195892 at epoch 7\n",
      "Error: 0.03425378104330062 at epoch 8\n",
      "Error: 0.03248987346396587 at epoch 9\n",
      "Error: 0.03038878859104696 at epoch 10\n",
      "Error: 0.027961703578979452 at epoch 11\n",
      "Error: 0.025260742501149774 at epoch 12\n",
      "Error: 0.0223814864062635 at epoch 13\n",
      "Error: 0.019452034389078204 at epoch 14\n",
      "Error: 0.016609403453979522 at epoch 15\n",
      "Error: 0.013972092834168056 at epoch 16\n",
      "Error: 0.011620460718326689 at epoch 17\n",
      "Error: 0.009591251408103702 at epoch 18\n",
      "Error: 0.007884307341589809 at epoch 19\n",
      "Error: 0.006474938341101697 at epoch 20\n",
      "Error: 0.005326050353008506 at epoch 21\n",
      "Error: 0.004397101255625077 at epoch 22\n",
      "Error: 0.003649472023426722 at epoch 23\n",
      "Error: 0.0030490447787365595 at epoch 24\n",
      "Error: 0.0025670048634835237 at epoch 25\n",
      "Error: 0.0021796815387313847 at epoch 26\n",
      "Error: 0.001867957505172067 at epoch 27\n",
      "Error: 0.0016165479581340663 at epoch 28\n",
      "Error: 0.0014132995708513248 at epoch 29\n",
      "Error: 0.0012485728048632519 at epoch 30\n",
      "Error: 0.001114725296800336 at epoch 31\n",
      "Error: 0.0010056925344878425 at epoch 32\n",
      "Error: 0.0009166534626651306 at epoch 33\n",
      "Error: 0.0008437665398532868 at epoch 34\n",
      "Error: 0.0007839625584900417 at epoch 35\n",
      "Error: 0.0007347824226812777 at epoch 36\n",
      "Error: 0.0006942501803641666 at epoch 37\n",
      "Error: 0.000660773551780672 at epoch 38\n",
      "Error: 0.0006330658516360352 at epoch 39\n",
      "Error: 0.0006100845500024618 at epoch 40\n",
      "Error: 0.0005909827863032071 at epoch 41\n",
      "Error: 0.0005750709862128964 at epoch 42\n",
      "Error: 0.0005617863783002794 at epoch 43\n",
      "Error: 0.00055066870577876 at epoch 44\n",
      "Error: 0.0005413408120159626 at epoch 45\n",
      "Error: 0.0005334930730113534 at epoch 46\n",
      "Error: 0.0005268708766203905 at epoch 47\n",
      "Error: 0.0005212645228887283 at epoch 48\n",
      "Error: 0.000516501054713258 at epoch 49\n",
      "\n",
      "Our network believes that 0.3 + 0.1 = [0.39739071]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #Creating a multi-layer perceptrion (mlp)\n",
    "    mlp = MLP(2, [5], 1)\n",
    "    \n",
    "    #Creating a dummy dataset\n",
    "    inputs = np.array([[random() / 2 for _ in range(2)] for _ in range(1000)]) #array example: ([[0.1, 0.2], [0.3, 0.4]])\n",
    "     #We are seeing if the network \"learns\" addition without us telling it.\n",
    "    targets = np.array([[i[0] +i[1]] for i in inputs]) #array example: ([[0.3], [0.7]])\n",
    "\n",
    "#Section is commented out given that it is now implemented in the def train() method.     \n",
    "#     #Doing forward propagation\n",
    "#     output = mlp.forward_propagate(inputs)\n",
    "    \n",
    "#     #Calculating error\n",
    "#     error = target - output\n",
    "    \n",
    "#     #Doing backward propagation\n",
    "#     mlp.back_propagate(error, verbose = False)\n",
    "    \n",
    "#     #Applying gradient descent\n",
    "#     mlp.gradient_descent(learning_rate = 1)\n",
    "\n",
    "    #Training the mlp\n",
    "    mlp.train(inputs, targets, 50, 0.1)\n",
    "    \n",
    "    \n",
    "    #Now finally using the model to make predictions\n",
    "    input = np.array([0.3, 0.1])\n",
    "    target = np.array([0.4])\n",
    "    \n",
    "    #Passing dummy data through the network to make a prediction.\n",
    "    output = mlp.forward_propagate(input) \n",
    "    print()\n",
    "    \n",
    "    #Printing predictions\n",
    "    print(\"Our network believes that {} + {} = {}\".format(input[0], input[1], output))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
